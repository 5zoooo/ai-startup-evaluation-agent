{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 365 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë”©\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Pinecone ì—°ê²°\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"investment-analysis-final\"\n",
    "\n",
    "# 3. ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # OpenAI ì„ë² ë”© ì°¨ì› ìˆ˜\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"  # ğŸ‘‰ ì‹¤ì œ Pinecone ì¸ë±ìŠ¤ ì§€ì—­\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 4. ì¸ë±ìŠ¤ ì—°ê²°\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# 5. OpenAI ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 6. ë¬¸ì„œ ë¶„í•  ì „ëµ ì •ì˜\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 7. PDF íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "pdf_files = [\n",
    "    (\"AI News.pdf\", \"ai_news\"),\n",
    "    (\"Startup Evaluation Metrics.pdf\", \"startup_metrics\"),\n",
    "    (\"VoyagerX Introduction.pdf\", \"voyagerx_intro\"),\n",
    "    (\"Video Stt Script.pdf\", \"video_stt_script\")  # âœ… ì¶”ê°€ëœ PDF\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# 8. ê° PDF ë¡œë”©, ë¶„í• , ë©”íƒ€ë°ì´í„° ë¶€ì—¬\n",
    "for filename, doc_id in pdf_files:\n",
    "    loader = PyPDFLoader(filename)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    for i, doc in enumerate(pages):\n",
    "        doc.metadata[\"source\"] = doc_id\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 9. Pineconeì— ì—…ë¡œë“œ\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embedding_model,\n",
    "    index_name=index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  GPT ë‹¨ë… ì‘ë‹µ:\n",
      " ë‚¨ì„¸ë™ CEOëŠ” í•œêµ­ì˜ ê¸°ì—…ì¸ìœ¼ë¡œ, ì£¼ì‹íšŒì‚¬ ë„¤ì´ë²„ì˜ í˜„ CEOì´ë©° ì´ì‚¬íšŒ ì˜ì¥ì´ê¸°ë„ í•©ë‹ˆë‹¤. ê·¸ëŠ” ë„¤ì´ë²„ì˜ ì°½ì—… ë©¤ë²„ ì¤‘ í•œ ëª…ìœ¼ë¡œ, íšŒì‚¬ì˜ ì„±ì¥ê³¼ ë°œì „ì— í° ì—­í• ì„ í•´ì™”ìŠµë‹ˆë‹¤. ë˜í•œ ì¸í„°ë„· ê¸°ì—…ì˜ ì„ êµ¬ìë¡œì„œ, í•œêµ­ì˜ IT ì‚°ì—…ì„ ì„ ë„í•˜ëŠ” ì¸ë¬¼ë¡œ í‰ê°€ë°›ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ë„¤ì´ë²„ë¥¼ í•œêµ­ì„ ë„˜ì–´ ê¸€ë¡œë²Œ ê¸°ì—…ìœ¼ë¡œ ì„±ì¥ì‹œí‚¤ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"ë‚¨ì„¸ë™ CEOì— ëŒ€í•´ ì•Œë ¤ì¤˜.\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt.format())\n",
    "print(\"ğŸ§  GPT ë‹¨ë… ì‘ë‹µ:\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ë²¡í„° DB + GPT ì‘ë‹µ:\n",
      " ë‚¨ì„¸ë™ì€ ë³´ì´ì €ì—‘ìŠ¤ì˜ CEOì´ë©°, ìê¸°ì£¼ë„ì ì¸ ì§ì›ì„ ì±„ìš©í•˜ê¸° ìœ„í•´ 4ë‹¨ê³„ì˜ ì±„ìš© ê³¼ì •ì„ ê±°ì¹˜ë©°, ì§€ì›ìì˜ ê³¼ê±° ê²½í—˜ê³¼ ì •ì§ì„±ì„ ì¤‘ìš”í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ, ì–¸ëŸ¬ë‹(Unlearning)ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, íšŒì‚¬ì™€ ê°œì¸ì´ ë™ë°˜ì˜ ê°œë…ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. íšŒì‚¬ì˜ ì„±ì¥ê³¼ ê°œì¸ì˜ ì„±ì¥ ì‚¬ì´ì˜ ì¡°í™”ì™€ ë°¸ëŸ°ìŠ¤ë¥¼ ë§ì¶”ëŠ” ê²ƒì„ ì¤‘ìš”ì‹œí•˜ê³  ìˆìœ¼ë©°, íšŒì‚¬ì— í—Œì‹ í•˜ì§€ ë§ë¼ê³  ì´ì•¼ê¸°í•˜ë©°, ë§¹ëª©ì ìœ¼ë¡œ ì§€ì‹œì— ë”°ë¥´ëŠ” ê²ƒì´ ì•„ë‹Œ ìê¸°ì£¼ë„ì ìœ¼ë¡œ ì¼í•˜ê³  ì„±ì¥í•  ìˆ˜ ìˆëŠ” ë¬¸í™”ë¥¼ ë§Œë“¤ì–´ê°€ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "query = \"ë‚¨ì„¸ë™ CEOì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜\"\n",
    "\n",
    "# ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ í¬í•¨\n",
    "final_prompt = f\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(final_prompt)\n",
    "print(\"ğŸ“š ë²¡í„° DB + GPT ì‘ë‹µ:\\n\", response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
